{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepHive: A multi-agent reinforcement learning approach for automated discovery of swarm-based optimization policies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use the deephive optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eikpon1\\Envs\\RLEnv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from commons.utils import get_config, get_environment, get_policy, get_obj_func\n",
    "from src.deephive import DeepHive\n",
    "from src.mappo import MAPPO\n",
    "from src.environment import OptimizationEnv\n",
    "import numpy as np\n",
    "from registry import Registry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the config file\n",
    "config = get_config('config.yml')\n",
    "environment_config = config['environment_config']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Environment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the environment parameters\n",
    "n_agents = environment_config['n_agents']\n",
    "n_dim = environment_config['n_dim']\n",
    "ep_length = environment_config['ep_length']\n",
    "freeze = environment_config['freeze']\n",
    "opt_bound = environment_config['opt_bound']\n",
    "reward_type = environment_config['reward_type']\n",
    "\n",
    "mode = 'test'\n",
    "title = 'tutorial02'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can chose to use already registered optimization environments or we can register a new one\n",
    "#### First let us use an already registered environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.51       0.25833333 0.        ]\n",
      " [0.44333333 0.1        0.48534057]\n",
      " [0.56       0.475      0.39053804]\n",
      " [0.38833332 0.37333333 0.3032356 ]\n",
      " [0.41166666 0.76       0.5547444 ]\n",
      " [0.915      0.5083333  0.31475282]\n",
      " [0.5516667  0.835      0.31438377]\n",
      " [0.95666665 0.15666667 0.53685486]\n",
      " [0.67       0.44833332 1.        ]\n",
      " [0.945      0.77       0.4532216 ]\n",
      " [0.79       0.33666667 0.15035988]\n",
      " [0.93666667 0.075      0.3412973 ]\n",
      " [0.58       0.41       0.5989853 ]\n",
      " [0.765      0.39       0.48835653]\n",
      " [0.50166667 0.735      0.6164073 ]\n",
      " [0.02333333 0.17       0.24716564]\n",
      " [0.45833334 0.16166666 0.55228686]\n",
      " [0.9533333  0.19166666 0.41279554]\n",
      " [0.7366667  0.06333333 0.5811763 ]\n",
      " [0.40166667 0.07833333 0.37029055]]\n"
     ]
    }
   ],
   "source": [
    "env_name = \"cos_function\"\n",
    "env = get_environment(config, env_name, reinit=True)\n",
    "state = env.reset()\n",
    "print(state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a trained policy\n",
    "Now, we are going to load a trained policy from `models/model.pth`. The MAPPO parameters are loaded from the config as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model from logs/cosmix_train/checkpoints/policy-900.pth...\n"
     ]
    }
   ],
   "source": [
    "# get the policy\n",
    "policy = get_policy(config, mode=mode)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize DeepHive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tutorial02 : TEST RUN SUMMARY\n",
      "Environment: cos_function\n",
      "Number of agents: 20\n",
      "Number of dimensions: 2\n",
      "Episode length: 20\n",
      "Number of runs: 5\n"
     ]
    }
   ],
   "source": [
    "deephive = DeepHive(title, env, policy, mode, config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize the Cosine Mixture Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************Run: 1/5***************\n",
      "Episode best: 8 | Number of best changes: 2 | Total runtime: 0.589931\n",
      "Global best values: [2.01363182 1.97887504] | Global best fitness: 5.993365657757597\n",
      "*************Run: 2/5***************\n",
      "Episode best: 19 | Number of best changes: 5 | Total runtime: 0.044102\n",
      "Global best values: [1.99473083 1.95775247] | Global best fitness: 5.981003630544476\n",
      "*************Run: 3/5***************\n",
      "Episode best: 9 | Number of best changes: 4 | Total runtime: 0.087461\n",
      "Global best values: [2.09557295 1.96208549] | Global best fitness: 5.889965444720485\n",
      "*************Run: 4/5***************\n",
      "Episode best: 13 | Number of best changes: 3 | Total runtime: 0.077795\n",
      "Global best values: [2.01736045 2.0590266 ] | Global best fitness: 5.9603907412244\n",
      "*************Run: 5/5***************\n",
      "Episode best: 13 | Number of best changes: 5 | Total runtime: 0.093013\n",
      "Global best values: [1.91119754 1.87063968] | Global best fitness: 5.745353480590932\n"
     ]
    }
   ],
   "source": [
    "deephive.optimize(debug=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering a New Environment\n",
    "In order to optimize a new function, we need to create the RL environment for it and then add it to registry or use it as it is. The steps are as follows:\n",
    "1. Create your optimization function and add it to  `commons.objective_functions.py`\n",
    "2. Specify the bounds, max value if known in the   `get_obj_func` method (see the examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"sphere\"\n",
    "function_info = get_obj_func(env_name)\n",
    "objfunc, bounds, opt_obj_value, type = function_info\n",
    "\n",
    "# create the environment\n",
    "env = OptimizationEnv(env_name=env_name, optFunc=objfunc, n_agents=n_agents, \n",
    "                      n_dim=n_dim, ep_length=ep_length, bounds=bounds, opt_value=opt_obj_value, \n",
    "                      reward_type=reward_type, freeze=freeze, opt_bound=opt_bound)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registry: \n",
    "The registry is the class that help organize the environments. We can add, get, delete environments from the registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cosine_mixture': <src.environment.OptimizationEnv object at 0x000002EB9206BEE0>, 'minmax': <src.environment.OptimizationEnv object at 0x000002EB92009490>, 'cos_function': <src.environment.OptimizationEnv object at 0x000002EB92009610>}\n"
     ]
    }
   ],
   "source": [
    "registry = Registry()\n",
    "# load the saved registry\n",
    "registry._load_envs(environment_config['envs_cache_path'])\n",
    "# get the list of registered environments\n",
    "env_names = registry._get_all_envs()\n",
    "# print the list of registered environments\n",
    "print(env_names)\n",
    "\n",
    "# add the environment to the registry\n",
    "registry.add_env(env_name, env)\n",
    "\n",
    "# save the registry\n",
    "registry._save_envs(environment_config['envs_cache_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tutorial02 : TEST RUN SUMMARY\n",
      "Environment: sphere\n",
      "Number of agents: 20\n",
      "Number of dimensions: 2\n",
      "Episode length: 20\n",
      "Number of runs: 5\n",
      "*************Run: 1/5***************\n",
      "Episode best: 8 | Number of best changes: 5 | Total runtime: 0.113657\n",
      "Global best values: [-0.02515161  0.00181425] | Global best fitness: -0.0006358961440915708\n",
      "*************Run: 2/5***************\n",
      "Episode best: 18 | Number of best changes: 2 | Total runtime: 0.081012\n",
      "Global best values: [-0.01804125 -0.02377224] | Global best fitness: -0.000890599966581101\n",
      "*************Run: 3/5***************\n",
      "Episode best: 8 | Number of best changes: 5 | Total runtime: 0.090838\n",
      "Global best values: [0.00902081 0.02332377] | Global best fitness: -0.0006253633230600064\n",
      "*************Run: 4/5***************\n",
      "Episode best: 9 | Number of best changes: 1 | Total runtime: 0.093654\n",
      "Global best values: [-0.02507079  0.03118658] | Global best fitness: -0.0016011571061642035\n",
      "*************Run: 5/5***************\n",
      "Episode best: 1 | Number of best changes: 3 | Total runtime: 0.089701\n",
      "Global best values: [-0.02981973 -0.0162878 ] | Global best fitness: -0.001154512475777316\n"
     ]
    }
   ],
   "source": [
    "# get the environment from the registry\n",
    "env = registry.get_env(env_name)\n",
    "\n",
    "# run optimization\n",
    "deephive = DeepHive(title, env, policy, mode, config)\n",
    "deephive.optimize(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
